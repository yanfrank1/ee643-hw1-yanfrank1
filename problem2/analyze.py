# -*- coding: utf-8 -*-
"""analyze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K7J8Z3AWLD3X65VzccHohVETHqNYrDXU
"""

import os
import json
import torch
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np

from dataset import create_test_dataloader
from model import create_model

def evaluate_model_on_length(model, data_path, device):
    loader = create_test_dataloader(data_path, batch_size=128)
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for sequences, labels, _ in loader:
            sequences, labels = sequences.to(device), labels.to(device)
            preds = model(sequences).argmax(dim=-1)
            correct += (preds == labels).sum().item()
            total += len(labels)
    return correct / total

def plot_extrapolation_curves(results_dict, test_lengths, save_path):
    plt.figure()

    for enc_type, accs in results_dict.items():
        plt.plot(test_lengths, accs)

    plt.xticks(test_lengths)
    plt.ylim(0, 1.05)
    plt.xlabel("Sequence Length")
    plt.ylabel("Accuracy")
    plt.title("Accuracy vs. Sequence Length")
    plt.grid(True, alpha=0.3)
    plt.savefig(save_path)
    plt.close()


def visualize_learned_pos_embeddings(model, save_path):
    embeddings = model.pos_encoding.position_embeddings.weight.detach().cpu().numpy()
    n_positions = min(embeddings.shape[0], 64)
    plt.figure()
    plt.imshow(embeddings[:n_positions], aspect="auto")
    plt.colorbar(label="Embedding value")
    plt.xlabel("Embedding Dimension")
    plt.ylabel("Position Index")
    plt.title("Learned Positional Embeddings")
    plt.savefig(save_path)
    plt.close()

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, "../data/problem2")
    results_dir = "results"
    extrapolation_dir = os.path.join(results_dir, "extrapolation")
    os.makedirs(extrapolation_dir, exist_ok=True)
    encoding_types = ["sinusoidal", "learned", "none"]
    test_lengths = [32, 64, 128, 256]

    extrapolation_results = {}
    for enc_type in encoding_types:
        model_path = os.path.join(results_dir, enc_type, "best_model.pth")
        checkpoint = torch.load(model_path, map_location=device)
        config = checkpoint["config"]

        model = create_model(
            encoding_type=enc_type,
            vocab_size=config["vocab_size"],
            d_model=config["d_model"],
            num_heads=config["num_heads"],
            num_layers=config["num_layers"],
            d_ff=config["d_ff"],
            dropout=config["dropout"],
            max_seq_len=config["max_seq_len"]
        ).to(device)
        
        
        model.load_state_dict(checkpoint["model_state_dict"])
        model.eval()

        length_accuracies = []
        for L in test_lengths:
            test_path = os.path.join(data_dir, f"test_len_{L}.jsonl")
            acc = evaluate_model_on_length(model, test_path, device)
            length_accuracies.append(acc)

        extrapolation_results[enc_type] = length_accuracies

        if enc_type == "learned":
            visualize_learned_pos_embeddings(model, os.path.join(extrapolation_dir, "learned_position_embeddings.png"))

    results_path = os.path.join(extrapolation_dir, "extrapolation_results.json")
    with open(results_path, "w") as f:
        json.dump({"test_lengths": test_lengths, "results": extrapolation_results}, f, indent=2)

    plot_extrapolation_curves(extrapolation_results, test_lengths, os.path.join(extrapolation_dir, "extrapolation_curves.png"))

if __name__ == "__main__":
    main()