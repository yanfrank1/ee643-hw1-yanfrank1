# -*- coding: utf-8 -*-
"""analyze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fkCR7cF6n6xQDzaqzwm-y_d-UCzzgIXb
"""

import os
import json
import torch
import torch.nn.functional as F
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from model import Seq2SeqTransformer
from dataset import create_dataloaders
from attention import create_causal_mask
from train import compute_masked_loss

def get_attention(model, src, tgt, device):
    src, tgt = src.to(device), tgt.to(device)
    tgt_input = tgt[:, :-1]
    tgt_mask = create_causal_mask(tgt_input.size(1), device=device)
    attention_maps = {}

    def save_attention_hook(name):
        def hook(module, input, output):
            _, attn_weights = output
            attention_maps[name] = attn_weights.detach().cpu()
        return hook

    for i, layer in enumerate(model.decoder_layers):
        layer.self_attn.register_forward_hook(save_attention_hook(f"decoder_self_{i}"))
        layer.cross_attn.register_forward_hook(save_attention_hook(f"decoder_cross_{i}"))

    _ = model(src, tgt_input, tgt_mask=tgt_mask)
    return attention_maps

def plot_attention(attention_maps, save_dir, src, tgt):
    os.makedirs(save_dir, exist_ok=True)
    cross_maps = {k: v for k, v in attention_maps.items() if "cross" in k}
    sample_key = next(iter(cross_maps))
    _, num_heads, _, _ = cross_maps[sample_key].shape
    global_heads = [ [] for _ in range(num_heads) ]
    for layer_name, attn in cross_maps.items():
        for h in range(num_heads):
            global_heads[h].append(attn[0, h])
    
    global_heads_mean = [torch.stack(maps).mean(0) for maps in global_heads]
    
    for h, mean_map in enumerate(global_heads_mean):
        plt.figure()
        sns.heatmap(
            mean_map,
            xticklabels=['+' if x.item() == 10 else str(x.item()) for x in src[0]],
            yticklabels=[str(x.item()) for x in tgt[0, 1:]],
            cbar=True
        )
        plt.title(f"Head{h} Visualization")
        plt.xlabel("Source")
        plt.ylabel("Target")
        plt.savefig(os.path.join(save_dir, f"attention_head{h}.png"))
        plt.close()

def visualize_testcase(attention_maps, src, tgt, i, save_dir="results/attention_patterns/testcase_visualizations"): 
    os.makedirs(save_dir, exist_ok=True)
    combined = []
    for layer_name, attn in attention_maps.items():
        if "cross" in layer_name:
            combined.append(attn)
    
    all_maps = torch.stack(combined)
    mean_map = all_maps.mean(dim=(0, 2))[0]
    
    plt.figure()
    sns.heatmap(
        mean_map, 
        xticklabels=['+' if (x.item()) == 10 else str(x.item()) for x in src[0]],
        yticklabels=[str(x.item()) for x in tgt[0, 1:]],
        cbar=True)
    plt.xlabel("Source")
    plt.ylabel("Target")
    plt.title(f"Example {i}: Combined Cross-Attention")
    plt.savefig(os.path.join(save_dir, f"example{i}_attention.png"))
    plt.close()
  
def ablate_attention_heads(model, src, tgt, device):
    src = src.to(device)
    tgt = tgt.to(device)
    model.eval()
    with torch.no_grad():
        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]
        tgt_mask = create_causal_mask(tgt_input.size(1), device=device)

        base_logits = model(src, tgt_input, tgt_mask=tgt_mask)
        base_loss = F.cross_entropy(
            base_logits.reshape(-1, base_logits.size(-1)),
            tgt_output.reshape(-1),
            ignore_index = 0
        ).item()
    
    num_heads = model.encoder_layers[0].self_attn.num_heads
    head_importance = []
    
    for h in range(num_heads):
        head_mask = torch.ones(num_heads, device=device)
        head_mask[h] = 0

        for layer in model.encoder_layers:
            layer.self_attn.head_mask = head_mask
        for layer in model.decoder_layers:
            layer.self_attn.head_mask = head_mask
            layer.cross_attn.head_mask = head_mask
        
        with torch.no_grad():
            logits = model(src, tgt_input, tgt_mask=tgt_mask)
            loss = F.cross_entropy(
                logits.reshape(-1, logits.size(-1)),
                tgt_output.reshape(-1),
                ignore_index = 0
            ).item()
            
        for layer in model.encoder_layers:
            layer.self_attn.head_mask = None
        for layer in model.decoder_layers:
            layer.self_attn.head_mask = None
            layer.cross_attn.head_mask = None
        
        importance = loss - base_loss
        head_importance.append(importance)

    return head_importance
            
def main():
    results_dir = "results"
    data_dir = "../data/problem1"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    attention_dir = os.path.join(results_dir, "attention_patterns")
    head_dir = os.path.join(results_dir, "head_analysis")
    os.makedirs(attention_dir, exist_ok=True)
    os.makedirs(head_dir, exist_ok=True)
    path = os.path.join(results_dir, "best_model.pth")
    checkpoint = torch.load(path, map_location=device)
    config = checkpoint["config"]

    model = Seq2SeqTransformer(
        vocab_size=config["vocab_size"],
        d_model=config["d_model"],
        num_heads=config["num_heads"],
        num_encoder_layers=config["num_encoder_layers"],
        num_decoder_layers=config["num_decoder_layers"],
        d_ff=config["d_ff"],
        dropout=config["dropout"]
    ).to(device)

    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()

    _, _, test_loader = create_dataloaders(data_dir, batch_size=1)
    for i, (src, tgt) in enumerate(test_loader):
        if i >= 5:
            break
        attention_maps = get_attention(model, src, tgt, device)
        plot_attention(attention_maps, attention_dir, src, tgt)
        visualize_testcase(attention_maps, src, tgt, i)

    src, tgt = next(iter(test_loader))
    importance = ablate_attention_heads(model, src, tgt, device)
    sorted_idx = np.argsort(importance)[::-1]
    results = {
        "head_importance": importance,
        "ranking": sorted_idx.tolist()
    }

    with open(os.path.join(head_dir, "head_importance.json"), "w") as f:
        json.dump(results, f, indent=2)

    plt.figure()
    plt.bar(np.arange(len(importance)), importance, color="teal")
    plt.title("Head Importance")
    plt.xlabel("Head index")
    plt.ylabel("Delta Loss")
    plt.savefig(os.path.join(head_dir, "head_importance.png"))
    plt.close()

if __name__ == "__main__":
    main()